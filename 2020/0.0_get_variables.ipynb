{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep variables for HydroTas 2020-2021 workplan:\n",
    "- Skill assessment\n",
    "  - rainfall, surface temp and surface wind over Australia region and Tasmania region\n",
    "  - Assess skill as function of start month and ensemble size\n",
    "- UNSEEN\n",
    "  - Tasmanian rainfall and Melbourne surface temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cftime\n",
    "import geopandas\n",
    "import regionmask\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import myfuncs as my\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import PBSCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "walltime = '01:00:00'\n",
    "cores = 24\n",
    "memory = '48GB'\n",
    "\n",
    "cluster = PBSCluster(processes=1,\n",
    "                     walltime=str(walltime), cores=cores, memory=str(memory),\n",
    "                     job_extra=['-l ncpus='+str(cores),\n",
    "                                '-l mem='+str(memory),\n",
    "                                '-P ux06',\n",
    "                                '-l jobfs=100GB',\n",
    "                                '-l storage=gdata/xv83+gdata/v14+scratch/v14'],\n",
    "                     local_directory='$PBS_JOBFS',\n",
    "                     header_skip=['select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.6.55.45:45537</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.6.55.45:8787/status' target='_blank'>http://10.6.55.45:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.6.55.45:45537' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.scale(jobs=2)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Australia land mask on CAFE grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f6(ds):\n",
    "    \"\"\" Preprocess steps for the CAFE-f6 forecasts\"\"\"\n",
    "    # Drop some coordinates\n",
    "    for drop_coord in ['average_DT', 'average_T1', 'average_T2', 'zsurf', 'area']:\n",
    "        if drop_coord in ds.coords:\n",
    "            ds = ds.drop(drop_coord)\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAFE_area = preprocess_f6(\n",
    "    my.open_zarr(\n",
    "        '/g/data/v14/vxk563/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-20201101/ZARR/atmos_isobaric_daily.zarr.zip',\n",
    "    )['area'])\n",
    "\n",
    "NRM = geopandas.read_file('data/NRM_clusters/NRM_clusters.shp')\n",
    "regions = regionmask.Regions_cls(\n",
    "    name='NRM_regions', \n",
    "    numbers=list(NRM.index), \n",
    "    names=list(NRM.label), \n",
    "    abbrevs=list(NRM.code),\n",
    "    outlines=list(NRM.geometry))\n",
    "regions_mask = regions.mask(CAFE_area, lon_name='lon', lat_name='lat')\n",
    "\n",
    "Australia_mask = xr.where(regions_mask.notnull(), True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = {'AUS': Australia_mask,\n",
    "           'TAS': (-42, 146.5),\n",
    "           'MEL': (-37.81, 144.96)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get f6 atmospheric monthly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'precip': 'precip', \n",
    "             't_ref': 't_ref',\n",
    "             'u_ref': 'u_ref', \n",
    "             'v_ref': 'v_ref'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_xv83 = glob.glob(\n",
    "    '/g/data/xv83/ds0092/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-??????01/ZARR/atmos_isobaric_month.zarr.zip'\n",
    ")\n",
    "paths_v14 = glob.glob(\n",
    "    '/g/data/v14/vxk563/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-??????01/ZARR/atmos_isobaric_month.zarr.zip'\n",
    ")\n",
    "paths = sorted(paths_xv83+paths_v14, key=lambda x: x.split('/')[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 7.28 s, total: 1min 19s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, region in REGIONS.items():\n",
    "    ds = my.open_zarr_forecasts(\n",
    "        paths, \n",
    "        variables=VARIABLES.keys(),\n",
    "        preprocess=preprocess_f6).rename(VARIABLES)#.chunk({'init_date':16})\n",
    "    \n",
    "    # Weighted mean over region\n",
    "    ds = my.get_region(ds, region).weighted(CAFE_area).mean(['lat','lon'])\n",
    "\n",
    "    # Fill nans in time with dummy times so that time operations work nicely\n",
    "    ds = ds.assign_coords({\n",
    "        'time': ds.time.fillna(cftime.DatetimeJulian(1800, 1, 1))})\n",
    "\n",
    "    # Chunk appropriately\n",
    "    ds = ds.chunk({'init_date': -1, 'lead_time': -1})\n",
    "    \n",
    "    # Save\n",
    "    [my.to_zarr(ds[[var]], f'./data/f6_{var}_{name}_raw.zarr')\n",
    "     for var in VARIABLES.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get f5 atmospheric monthly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'precip': 'precip', \n",
    "             't_ref': 't_ref',\n",
    "             'u_ref': 'u_ref', \n",
    "             'v_ref': 'v_ref'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/vxk563/CAFE/forecasts/f5/ZARR/atmos_isobaric_month.zarr.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f5(ds):\n",
    "    \"\"\" Preprocess steps for the CAFE-f6 forecasts\"\"\"\n",
    "    # Drop some coordinates\n",
    "    for drop_coord in ['average_DT', 'average_T1', 'average_T2', 'zsurf', 'area']:\n",
    "        if drop_coord in ds.coords:\n",
    "            ds = ds.drop(drop_coord)\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 1.38 s, total: 16.6 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, region in REGIONS.items():\n",
    "    ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(),\n",
    "        preprocess=preprocess_f5).rename(VARIABLES)\n",
    "    \n",
    "    # Weighted mean over region\n",
    "    ds = my.get_region(ds, region).weighted(CAFE_area).mean(['lat','lon'])\n",
    "\n",
    "    # Fill nans in time with dummy times so that time operations work nicely\n",
    "    ds.time.attrs['calendar_type'] = 'JULIAN'\n",
    "    ds = ds.assign_coords({\n",
    "        'time': ds.time.fillna(cftime.DatetimeJulian(1800, 1, 1))})\n",
    "\n",
    "    # Chunk appropriately\n",
    "    ds = ds.chunk({'init_date': -1, 'lead_time': -1})\n",
    "    \n",
    "    # Save\n",
    "    [my.to_zarr(ds[[var]], f'./data/f5_{var}_{name}_raw.zarr')\n",
    "     for var in VARIABLES.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv_lead_times = ds.lead_time\n",
    "obsv_init_dates = ds.init_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JRA-55 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'TPRAT_GDS0_SFC': 'precip', \n",
    "             'TMP_GDS0_HTGL': 't_ref', \n",
    "             'UGRD_GDS0_HTGL': 'u_ref', \n",
    "             'VGRD_GDS0_HTGL': 'v_ref'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/ds0092/data/ZARR/csiro-dcfp-jra55/surface_month_cafe-grid.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_jra(ds):\n",
    "    \"\"\" Preprocess steps for the JRA data\"\"\"\n",
    "    # Rename time and level\n",
    "    for key, val in {'initial_time0_hours': 'time', \n",
    "                     'lv_ISBL1': 'level'}.items():\n",
    "        if key in ds.coords:\n",
    "                ds = ds.rename({key: val})\n",
    "    # Drop filename attribute\n",
    "    del ds.attrs['filename']\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.81 s, sys: 692 ms, total: 7.5 s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, region in REGIONS.items():\n",
    "    ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(), \n",
    "        preprocess=preprocess_jra).rename(VARIABLES)\n",
    "    \n",
    "    # Weighted mean over region\n",
    "    ds = my.get_region(ds, region).weighted(CAFE_area).mean(['lat','lon'])\n",
    "\n",
    "    # Chunk appropriately\n",
    "    ds = ds.chunk({'time': -1})\n",
    "    \n",
    "    # Save\n",
    "    [my.to_zarr(ds[[var]], f'./data/jra55_{var}_{name}_ts.zarr')\n",
    "     for var in VARIABLES.values()]\n",
    "    \n",
    "    # Stack by initial date\n",
    "    ds = my.stack_by_init_date(\n",
    "        ds, \n",
    "        obsv_init_dates, \n",
    "        len(obsv_lead_times)).chunk(\n",
    "        {'init_date': -1, 'lead_time': -1})\n",
    "    \n",
    "    # Fill nans in time with dummy times so that time operations work nicely\n",
    "    ds.time.attrs['calendar_type'] = 'Proleptic_Gregorian'\n",
    "    ds = ds.assign_coords({\n",
    "        'time': ds.time.fillna(cftime.DatetimeProlepticGregorian(1800, 1, 1))})\n",
    "    \n",
    "    # Save\n",
    "    [my.to_zarr(ds[[var]], f'./data/jra55_{var}_{name}.zarr')\n",
    "     for var in VARIABLES.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWAP monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'precip': 'precip'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/ds0092/data/ZARR/csiro-dcfp-csiro-awap/rain_day_19000101-20201202_cafe-grid.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_awap(ds):\n",
    "    \"\"\" Preprocess steps for the AWAP data\"\"\"\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.8 s, sys: 1.27 s, total: 1min 1s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, region in REGIONS.items():\n",
    "    ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(), \n",
    "        preprocess=preprocess_awap).rename(VARIABLES)\n",
    "    \n",
    "    # Sum to monthly values\n",
    "    def sum_min_samples(ds, dim, min_samples):\n",
    "        \"\"\" Return sum only if there are more than min_samples along dim \"\"\"\n",
    "        s = ds.sum(dim, skipna=False)\n",
    "        return s if len(ds[dim]) >= min_samples else np.nan*s\n",
    "    ds = ds.resample(time='MS').map(sum_min_samples, dim='time', min_samples=28)\n",
    "    \n",
    "    # Weighted mean over region\n",
    "    ds = my.get_region(ds, region).weighted(CAFE_area).mean(['lat','lon'])\n",
    "\n",
    "    # Chunk appropriately\n",
    "    ds = ds.chunk({'time': -1})\n",
    "    \n",
    "    # Save\n",
    "    [my.to_zarr(ds[[var]], f'./data/awap_{var}_{name}_ts.zarr')\n",
    "     for var in VARIABLES.values()]\n",
    "    \n",
    "    # Stack by initial date\n",
    "    ds = my.stack_by_init_date(\n",
    "        ds, \n",
    "        obsv_init_dates, \n",
    "        len(obsv_lead_times)).chunk(\n",
    "        {'init_date': -1, 'lead_time': -1})\n",
    "    \n",
    "    # Fill nans in time with dummy times so that time operations work nicely\n",
    "    ds.time.attrs['calendar_type'] = 'Proleptic_Gregorian'\n",
    "    ds = ds.assign_coords({\n",
    "        'time': ds.time.fillna(cftime.DatetimeProlepticGregorian(1800, 1, 1))})\n",
    "    \n",
    "    # Save\n",
    "    [my.to_zarr(ds[[var]], f'./data/awap_{var}_{name}.zarr')\n",
    "     for var in VARIABLES.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
