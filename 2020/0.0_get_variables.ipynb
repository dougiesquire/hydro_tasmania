{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep variables for HydroTas 2020-2021 workplan:\n",
    "- Skill assessment\n",
    "  - rainfall, surface temp and surface wind over Australia region and Tasmania region\n",
    "  - Assess skill as function of start month and ensemble size\n",
    "- UNSEEN\n",
    "  - Tasmanian rainfall and Melbourne surface temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cftime\n",
    "import geopandas\n",
    "import regionmask\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import myfuncs as my\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import PBSCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v14/ds0092/software/miniconda3/envs/pangeo/lib/python3.9/site-packages/distributed/node.py:160: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37757 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "walltime = '01:00:00'\n",
    "cores = 24\n",
    "memory = '48GB'\n",
    "\n",
    "cluster = PBSCluster(processes=1,\n",
    "                     walltime=str(walltime), cores=cores, memory=str(memory),\n",
    "                     job_extra=['-l ncpus='+str(cores),\n",
    "                                '-l mem='+str(memory),\n",
    "                                '-P ux06',\n",
    "                                '-l jobfs=100GB',\n",
    "                                '-l storage=gdata/xv83+gdata/v14+scratch/v14'],\n",
    "                     local_directory='$PBS_JOBFS',\n",
    "                     header_skip=['select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.6.63.72:40935</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.6.63.72:37757/status' target='_blank'>http://10.6.63.72:37757/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.6.63.72:40935' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.scale(jobs=2)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Australia land mask on CAFE grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f6_atmos(ds):\n",
    "    \"\"\" Preprocess steps for the CAFE-f6 atmos forecasts\"\"\"\n",
    "    # Drop some coordinates\n",
    "    for drop_coord in ['average_DT', 'average_T1', 'average_T2', 'zsurf', 'area']:\n",
    "        if drop_coord in ds.coords:\n",
    "            ds = ds.drop(drop_coord)\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAFE_area = preprocess_f6_atmos(\n",
    "    my.open_zarr(\n",
    "        '/g/data/v14/vxk563/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-20201101/ZARR/atmos_isobaric_daily.zarr.zip',\n",
    "    )['area'])\n",
    "\n",
    "NRM = geopandas.read_file('data/NRM_clusters/NRM_clusters.shp')\n",
    "regions = regionmask.Regions(\n",
    "    name='NRM_regions', \n",
    "    numbers=list(NRM.index), \n",
    "    names=list(NRM.label), \n",
    "    abbrevs=list(NRM.code),\n",
    "    outlines=list(NRM.geometry))\n",
    "regions_mask = regions.mask(CAFE_area, lon_name='lon', lat_name='lat')\n",
    "\n",
    "Australia_mask = xr.where(regions_mask.notnull(), True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = {'AUS': Australia_mask,\n",
    "           'TAS': (-42, 146.5),\n",
    "           'MEL': (-37.81, 144.96)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get f6 atmospheric monthly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'precip': \n",
    "                 {'name': 'precip', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             't_ref': \n",
    "                 {'name': 't_ref',\n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'u_ref': \n",
    "                 {'name': 'u_ref',\n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'v_ref': \n",
    "                 {'name': 'v_ref',\n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'h500':\n",
    "                 {'name': 'h500',\n",
    "                  'regions': ['NATIVE']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_xv83 = glob.glob(\n",
    "    '/g/data/xv83/ds0092/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-??????01/ZARR/atmos_isobaric_month.zarr.zip'\n",
    ")\n",
    "paths_v14 = glob.glob(\n",
    "    '/g/data/v14/vxk563/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-??????01/ZARR/atmos_isobaric_month.zarr.zip'\n",
    ")\n",
    "paths = sorted(paths_xv83+paths_v14, key=lambda x: x.split('/')[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting precip over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting t_ref over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting u_ref over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting v_ref over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting h500 over region(s) ['NATIVE']...\n",
      "CPU times: user 1min 47s, sys: 41.5 s, total: 2min 29s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = my.open_zarr_forecasts(\n",
    "        paths, \n",
    "        variables=VARIABLES.keys(),\n",
    "        preprocess=preprocess_f6_atmos\n",
    ").rename(\n",
    "    {k: v['name'] for k, v in VARIABLES.items()})\n",
    "\n",
    "for v_ in VARIABLES:\n",
    "    v = VARIABLES[v_]['name']\n",
    "    var = ds[[v]]\n",
    "    regions = VARIABLES[v_]['regions']\n",
    "    print(f'Extracting {v_} over region(s) {regions}...')\n",
    "    \n",
    "    for r in regions:\n",
    "        # Weighted mean over region\n",
    "        if r == 'NATIVE':\n",
    "            var_region = var\n",
    "        else:\n",
    "            var_region = my.get_region(\n",
    "                var, REGIONS[r]).weighted(\n",
    "                CAFE_area).mean(\n",
    "                ['lat','lon'])\n",
    "            \n",
    "            # Chunk appropriately\n",
    "            var_region = var_region.chunk({'init_date': -1, 'lead_time': -1})\n",
    "        \n",
    "        # Fill nans in time with dummy times so that time operations work nicely\n",
    "        var_region = var_region.assign_coords({\n",
    "            'time': var_region.time.fillna(cftime.DatetimeJulian(1800, 1, 1))})\n",
    "\n",
    "        # Save\n",
    "        my.to_zarr(var_region, f'./data/f6_{v}_{r}_raw.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f6_lead_times = ds.lead_time\n",
    "f6_init_dates = ds.init_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get f6 ocean monthly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'sst': \n",
    "                 {'name': 'sst', \n",
    "                  'regions': ['NATIVE']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_xv83 = glob.glob(\n",
    "    '/g/data/xv83/ds0092/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-??????01/ZARR/ocean_month.zarr.zip'\n",
    ")\n",
    "paths_v14 = glob.glob(\n",
    "    '/g/data/v14/vxk563/CAFE/forecasts/f6/WIP/c5-d60-pX-f6-??????01/ZARR/ocean_month.zarr.zip'\n",
    ")\n",
    "paths = sorted(paths_xv83+paths_v14, key=lambda x: x.split('/')[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f6_ocean(ds):\n",
    "    \"\"\" Preprocess steps for the CAFE-f6 ocean forecasts\"\"\"\n",
    "    # Drop some coordinates\n",
    "    for drop_coord in ['average_DT', 'average_T1', 'average_T2', 'geolat_t', 'geolon_t', 'area_t']:\n",
    "        if drop_coord in ds.coords:\n",
    "            ds = ds.drop(drop_coord)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sst over region(s) ['NATIVE']...\n",
      "CPU times: user 5min 17s, sys: 5min 39s, total: 10min 56s\n",
      "Wall time: 15min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = my.open_zarr_forecasts(\n",
    "        paths, \n",
    "        variables=VARIABLES.keys(),\n",
    "        preprocess=preprocess_f6_ocean\n",
    ").rename(\n",
    "    {k: v['name'] for k, v in VARIABLES.items()})\n",
    "\n",
    "for v_ in VARIABLES:\n",
    "    v = VARIABLES[v_]['name']\n",
    "    var = ds[[v]]\n",
    "    regions = VARIABLES[v_]['regions']\n",
    "    print(f'Extracting {v_} over region(s) {regions}...')\n",
    "    \n",
    "    for r in regions:\n",
    "        # Weighted mean over region\n",
    "        if r == 'NATIVE':\n",
    "            var_region = var\n",
    "        else:\n",
    "            var_region = my.get_region(\n",
    "                var, REGIONS[r]).weighted(\n",
    "                CAFE_aarea).mean(\n",
    "                ['lat','lon'])\n",
    "            \n",
    "            # Chunk appropriately\n",
    "            var_region = var_region.chunk({'init_date': -1, 'lead_time': -1})\n",
    "        \n",
    "        # Fill nans in time with dummy times so that time operations work nicely\n",
    "        var_region = var_region.assign_coords({\n",
    "            'time': var_region.time.fillna(cftime.DatetimeJulian(1800, 1, 1))})\n",
    "\n",
    "        # Save\n",
    "        my.to_zarr(var_region, f'./data/f6_{v}_{r}_raw.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get f5 atmospheric monthly variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'precip': \n",
    "                 {'name': 'precip', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             't_ref': \n",
    "                 {'name': 't_ref',\n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'u_ref': \n",
    "                 {'name': 'u_ref',\n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'v_ref': \n",
    "                 {'name': 'v_ref',\n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'h500':\n",
    "                 {'name': 'h500',\n",
    "                  'regions': ['NATIVE']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/vxk563/CAFE/forecasts/f5/ZARR/atmos_isobaric_month.zarr.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f5(ds):\n",
    "    \"\"\" Preprocess steps for the CAFE-f6 forecasts\"\"\"\n",
    "    # Drop some coordinates\n",
    "    for drop_coord in ['average_DT', 'average_T1', 'average_T2', 'zsurf', 'area']:\n",
    "        if drop_coord in ds.coords:\n",
    "            ds = ds.drop(drop_coord)\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting precip over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting t_ref over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting u_ref over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting v_ref over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "Extracting h500 over region(s) ['NATIVE']...\n",
      "CPU times: user 30.5 s, sys: 5.52 s, total: 36 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(),\n",
    "        preprocess=preprocess_f5).rename(\n",
    "    {k: v['name'] for k, v in VARIABLES.items()})\n",
    "\n",
    "for v_ in VARIABLES:\n",
    "    v = VARIABLES[v_]['name']\n",
    "    var = ds[[v]]\n",
    "    regions = VARIABLES[v_]['regions']\n",
    "    print(f'Extracting {v_} over region(s) {regions}...')\n",
    "    \n",
    "    for r in regions:\n",
    "        # Weighted mean over region\n",
    "        if r == 'NATIVE':\n",
    "            var_region = var\n",
    "        else:\n",
    "            var_region = my.get_region(\n",
    "                var, REGIONS[r]).weighted(\n",
    "                CAFE_area).mean(\n",
    "                ['lat','lon'])\n",
    "            \n",
    "            # Chunk appropriately\n",
    "            var_region = var_region.chunk({'init_date': -1, 'lead_time': -1})\n",
    "        \n",
    "        # Fill nans in time with dummy times so that time operations work nicely\n",
    "        var_region.time.attrs['calendar_type'] = 'JULIAN'\n",
    "        var_region = var_region.assign_coords({\n",
    "            'time': var_region.time.fillna(cftime.DatetimeJulian(1800, 1, 1))})\n",
    "\n",
    "        # Save\n",
    "        my.to_zarr(var_region, f'./data/f5_{v}_{r}_raw.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f5_lead_times = ds.lead_time\n",
    "f5_init_dates = ds.init_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv_lead_times = xr.concat([f5_lead_times, f6_lead_times], dim='lead_time')\n",
    "obsv_lead_times = obsv_lead_times[np.unique(obsv_lead_times, return_index=True)[1]]\n",
    "\n",
    "obsv_init_dates = xr.concat([f5_init_dates, f6_init_dates], dim='init_date')\n",
    "obsv_init_dates = obsv_init_dates[np.unique(obsv_init_dates, return_index=True)[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JRA-55 surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'TPRAT_GDS0_SFC': \n",
    "                 {'name': 'precip', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'TMP_GDS0_HTGL': \n",
    "                 {'name': 't_ref', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'UGRD_GDS0_HTGL': \n",
    "                 {'name': 'u_ref', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']},\n",
    "             'VGRD_GDS0_HTGL': \n",
    "                 {'name': 'v_ref', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/ds0092/data/ZARR/csiro-dcfp-jra55/surface_month_cafe-grid.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_jra(ds):\n",
    "    \"\"\" Preprocess steps for the JRA data\"\"\"\n",
    "    # Rename time and level\n",
    "    for key, val in {'initial_time0_hours': 'time', \n",
    "                     'lv_ISBL1': 'level'}.items():\n",
    "        if key in ds.coords:\n",
    "                ds = ds.rename({key: val})\n",
    "    # Drop filename attribute\n",
    "    del ds.attrs['filename']\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 ms, sys: 4.28 ms, total: 37.5 ms\n",
      "Wall time: 52.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(), \n",
    "        preprocess=preprocess_jra).rename(\n",
    "    {k: v['name'] for k, v in VARIABLES.items()})\n",
    "\n",
    "for v_ in VARIABLES:\n",
    "    v = VARIABLES[v_]['name']\n",
    "    var = ds[[v]]\n",
    "    regions = VARIABLES[v_]['regions']\n",
    "    print(f'Extracting {v_} over region(s) {regions}...')\n",
    "    \n",
    "    for r in regions:\n",
    "        # Weighted mean over region\n",
    "        if r == 'NATIVE':\n",
    "            var_region = var\n",
    "        else:\n",
    "            var_region = my.get_region(\n",
    "                var, REGIONS[r]).weighted(\n",
    "                CAFE_area).mean(\n",
    "                ['lat','lon'])\n",
    "            \n",
    "            # Chunk appropriately\n",
    "            var_region = var_region.chunk({'time': -1})\n",
    "            \n",
    "        # Stack by initial date\n",
    "        var_stacked = my.stack_by_init_date(\n",
    "            var_region, \n",
    "            obsv_init_dates, \n",
    "            len(obsv_lead_times)).chunk(\n",
    "            {'init_date': -1, 'lead_time': -1})\n",
    "        \n",
    "        # Fill nans in time with dummy times so that time operations work nicely\n",
    "        var_stacked.time.attrs['calendar_type'] = 'Proleptic_Gregorian'\n",
    "        var_stacked = var_stacked.assign_coords({\n",
    "            'time': var_stacked.time.fillna(cftime.DatetimeProlepticGregorian(1800, 1, 1))})\n",
    "\n",
    "        # Save\n",
    "        my.to_zarr(var_region, f'./data/jra55_{v}_{r}_ts.zarr')\n",
    "        my.to_zarr(var_stacked, f'./data/jra55_{v}_{r}.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWAP monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'precip': \n",
    "                 {'name': 'precip', \n",
    "                  'regions': ['AUS', 'TAS', 'MEL']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/ds0092/data/ZARR/csiro-dcfp-csiro-awap/rain_day_19000101-20201202_cafe-grid.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_awap(ds):\n",
    "    \"\"\" Preprocess steps for the AWAP data\"\"\"\n",
    "    # Truncate latitudes to 10dp\n",
    "    for dim in ds.dims:\n",
    "        if 'lat' in dim:\n",
    "            ds = ds.assign_coords({dim: ds[dim].round(decimals=10)})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting precip over region(s) ['AUS', 'TAS', 'MEL']...\n",
      "CPU times: user 47.2 s, sys: 1.74 s, total: 49 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(), \n",
    "        preprocess=preprocess_awap).rename(\n",
    "    {k: v['name'] for k, v in VARIABLES.items()})\n",
    "\n",
    "# Sum to monthly values\n",
    "def sum_min_samples(ds, dim, min_samples):\n",
    "    \"\"\" Return sum only if there are more than min_samples along dim \"\"\"\n",
    "    s = ds.sum(dim, skipna=False)\n",
    "    return s if len(ds[dim]) >= min_samples else np.nan*s\n",
    "ds = ds.resample(time='MS').map(sum_min_samples, dim='time', min_samples=28)\n",
    "\n",
    "for v_ in VARIABLES:\n",
    "    v = VARIABLES[v_]['name']\n",
    "    var = ds[[v]]\n",
    "    regions = VARIABLES[v_]['regions']\n",
    "    print(f'Extracting {v_} over region(s) {regions}...')\n",
    "    \n",
    "    for r in regions:\n",
    "        # Weighted mean over region\n",
    "        if r == 'NATIVE':\n",
    "            var_region = var\n",
    "        else:\n",
    "            var_region = my.get_region(\n",
    "                var, REGIONS[r]).weighted(\n",
    "                CAFE_area).mean(\n",
    "                ['lat','lon'])\n",
    "            \n",
    "            # Chunk appropriately\n",
    "            var_region = var_region.chunk({'time': -1})\n",
    "            \n",
    "        # Stack by initial date\n",
    "        var_stacked = my.stack_by_init_date(\n",
    "            var_region, \n",
    "            obsv_init_dates, \n",
    "            len(obsv_lead_times)).chunk(\n",
    "            {'init_date': -1, 'lead_time': -1})\n",
    "        \n",
    "        # Fill nans in time with dummy times so that time operations work nicely\n",
    "        var_stacked.time.attrs['calendar_type'] = 'Proleptic_Gregorian'\n",
    "        var_stacked = var_stacked.assign_coords({\n",
    "            'time': var_stacked.time.fillna(cftime.DatetimeProlepticGregorian(1800, 1, 1))})\n",
    "\n",
    "        # Save\n",
    "        my.to_zarr(var_region, f'./data/awap_{v}_{r}_ts.zarr')\n",
    "        my.to_zarr(var_stacked, f'./data/awap_{v}_{r}.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HadISST monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIABLES = {'sst': \n",
    "                 {'name': 'sst', \n",
    "                  'regions': ['NATIVE']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/g/data/v14/ds0092/data/ZARR/csiro-dcfp-hadisst/ocean_month.zarr.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cftime\n",
    "def preprocess_had(ds):\n",
    "    \"\"\" Preprocess steps for the AWAP data\"\"\"\n",
    "    # Truncate time to start of month\n",
    "    truncated_time = [\n",
    "        cftime.DatetimeGregorian(t.year, t.month, 1) \n",
    "        for t in ds.time.values]\n",
    "    return ds.assign_coords({'time': truncated_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sst over region(s) ['NATIVE']...\n",
      "CPU times: user 4.77 s, sys: 2.62 s, total: 7.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = my.open_zarr(\n",
    "        path, \n",
    "        variables=VARIABLES.keys(), \n",
    "        preprocess=preprocess_had).rename(\n",
    "    {k: v['name'] for k, v in VARIABLES.items()})\n",
    "\n",
    "for v_ in VARIABLES:\n",
    "    v = VARIABLES[v_]['name']\n",
    "    var = ds[[v]]\n",
    "    regions = VARIABLES[v_]['regions']\n",
    "    print(f'Extracting {v_} over region(s) {regions}...')\n",
    "    \n",
    "    for r in regions:\n",
    "        # Weighted mean over region\n",
    "        if r == 'NATIVE':\n",
    "            var_region = var\n",
    "        else:\n",
    "            var_region = my.get_region(\n",
    "                var, REGIONS[r]).weighted(\n",
    "                CAFE_area).mean(\n",
    "                ['lat','lon'])\n",
    "            \n",
    "            # Chunk appropriately\n",
    "            var_region = var_region.chunk({'time': -1})\n",
    "            \n",
    "        # Stack by initial date\n",
    "        var_stacked = my.stack_by_init_date(\n",
    "            var_region, \n",
    "            obsv_init_dates, \n",
    "            len(obsv_lead_times)).chunk(\n",
    "            {'lead_time': -1})\n",
    "        \n",
    "        # Fill nans in time with dummy times so that time operations work nicely\n",
    "        var_stacked.time.attrs['calendar_type'] = 'Gregorian'\n",
    "        var_stacked = var_stacked.assign_coords({\n",
    "            'time': var_stacked.time.fillna(cftime.DatetimeGregorian(1800, 1, 1))})\n",
    "\n",
    "        # Save\n",
    "        my.to_zarr(var_region, f'./data/had_{v}_{r}_ts.zarr')\n",
    "        my.to_zarr(var_stacked, f'./data/had_{v}_{r}.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
